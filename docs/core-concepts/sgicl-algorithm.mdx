---
title: "SGICL Algorithm"
description: "Understand how Self-Generated In-Context Learning enables agents to bootstrap their own performance"
---

## Overview

The **Self-Generated In-Context Learning** (SGICL) algorithm enables your LLM agents to bootstrap their own performance by learning from successful trajectories. Unlike traditional approaches that require you to craft demonstrations manually, SGICL allows agents to generate their own examples through trial and interaction.

## The core insight

LLMs perform significantly better when you provide relevant examples in their context (in-context learning). SGICL leverages this by:

1. **Self-generating examples**: Your agent creates its own demonstrations by attempting tasks
2. **Selective storage**: Only successful trajectories are stored as future examples
3. **Semantic retrieval**: The most relevant examples are retrieved for each new situation
4. **Continuous improvement**: Performance improves as the agent accumulates more successful experiences

## Algorithm phases

### 1. Bootstrap phase

During the initial bootstrap phase, your agent attempts tasks without prior examples:

```
Episode 1: Agent attempts task → Fails → Nothing stored
Episode 2: Agent attempts task → Succeeds → Trajectory stored
Episode 3: Agent attempts task → Succeeds → Trajectory stored
...
```

<Note>
Early episodes may have lower success rates, but each success contributes to improved future performance.
</Note>

### 2. Retrieval phase

At each decision point, the agent retrieves semantically similar trajectories:

```python
# During planning
examples = retriever.retrieve_for_plan(goal)

# During each step
examples = retriever.retrieve_for_step(goal, plan, observation)
```

The retrieval system uses:
- **Sentence Transformers** (`all-MiniLM-L6-v2` by default) for embedding
- **FAISS** for fast vector similarity search
- Configurable `k` parameter to control how many examples to retrieve

### 3. Generation phase

The LLM generates plans, reasoning, and actions informed by retrieved examples:

```
┌─────────────────────────────────────────────────────────────────┐
│ Goal: Navigate to /home/user and find config.json              │
│                                                                 │
│ Examples of similar successful tasks:                           │
│ ─────────────────────────────────────────────────────────────── │
│ Example 1: Goal: Find database.json in /home...                │
│   Steps: cd /home → ls → cd user → find database.json          │
│                                                                 │
│ Example 2: Goal: Locate settings in home directory...          │
│   Steps: cd /home/user → ls → cat settings.json                │
│ ─────────────────────────────────────────────────────────────── │
│                                                                 │
│ Based on these examples, generate a plan...                     │
└─────────────────────────────────────────────────────────────────┘
```

### 4. Curation phase

Low-utility trajectories are automatically pruned:

```python
# A trajectory is pruned when:
# 1. It has been retrieved at least `min_retrievals` times
# 2. Its utility score falls below `threshold`

utility_score = times_led_to_success / times_retrieved

if utility_score < threshold and times_retrieved >= min_retrievals:
    remove_trajectory(trajectory_id)
```

## Key benefits

<CardGroup cols={2}>
  <Card title="No human demonstrations" icon="robot">
    Agents learn from their own experience, eliminating the need for manual example curation
  </Card>
  <Card title="Domain adaptation" icon="bullseye">
    Examples naturally align with the specific tasks and environments your agent encounters
  </Card>
  <Card title="Continuous learning" icon="chart-line">
    Performance improves over time as more successful trajectories accumulate
  </Card>
  <Card title="Quality control" icon="filter">
    Automatic curation ensures only high-utility examples are retained
  </Card>
</CardGroup>

## Implementation in ICICL

ICICL implements SGICL with a clean API:

```python agent_setup.py
from icicl import Agent, LiteLLMProvider

agent = Agent(
    llm=LiteLLMProvider(model="gpt-4o-mini"),
    db_path="./trajectories",
    plan_prompt="...",
    reason_prompt="...",
    act_prompt="...",
    k=3,                        # Examples to retrieve
    curation_threshold=0.3,     # Utility threshold
    curation_min_retrievals=5,  # Min retrievals before pruning
)

# Training mode - stores successful trajectories
trajectory = await agent.train(env, goal="Complete task")

# Inference mode - uses examples, doesn't store new ones
trajectory = await agent.run(env, goal="Another task")
```

## Comparison with traditional ICL

| Aspect | Traditional ICL | Self-Generated ICL |
|--------|-----------------|-------------------|
| **Example source** | Human-crafted | Self-generated |
| **Scalability** | Limited by human effort | Scales automatically |
| **Domain alignment** | May be misaligned | Naturally aligned |
| **Maintenance** | Manual updates needed | Self-maintaining |
| **Cold start** | Requires examples upfront | Bootstraps from zero |

## Research foundation

ICICL is based on the algorithm described in:

> **Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks**

<Tip>
Key findings from the research:
- Self-generated examples can match or exceed human-crafted demonstrations
- Semantic retrieval significantly outperforms random example selection
- Automatic curation improves example quality over time
- The approach generalizes across different task domains
</Tip>

## Next steps

<CardGroup cols={2}>
  <Card title="ReAct Loop" icon="rotate" href="/core-concepts/react-loop">
    Learn how the Plan-Reason-Act execution loop works
  </Card>
  <Card title="Trajectory Database" icon="database" href="/core-concepts/trajectory-database">
    Understand how trajectories are stored and retrieved
  </Card>
</CardGroup>
