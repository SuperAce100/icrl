---
title: LLMProvider Protocol
description: "Protocol for LLM integrations"
---

## Overview

The `LLMProvider` protocol defines the interface for LLM integrations. Implement this protocol to use custom LLM backends with ICICL.

```python
from icicl import LLMProvider
```

## Protocol Definition

```python
from typing import Protocol, runtime_checkable
from icicl import Message

@runtime_checkable
class LLMProvider(Protocol):
    async def complete(self, messages: list[Message]) -> str:
        """Generate a completion from the given messages."""
        ...
```

## Methods

### complete

```python
async def complete(self, messages: list[Message]) -> str
```

Generate a completion from the given messages.

<ParamField path="messages" type="list[Message]" required>
  A list of `Message` objects representing the conversation. Each message has `role` and `content` fields.
</ParamField>

<ResponseField name="return" type="str">
  The generated completion as a string.
</ResponseField>

## Built-in Implementation

ICICL provides `LiteLLMProvider` which supports 100+ models:

```python
from icicl import LiteLLMProvider

llm = LiteLLMProvider(
    model="gpt-4o-mini",
    temperature=0.7,
    max_tokens=1000,
)

response = await llm.complete([
    Message(role="user", content="Hello!")
])
```

See [LiteLLMProvider](/api-reference/litellm-provider) for full documentation.

## Implementing Custom Providers

### Basic Implementation

```python
from icicl import Message

class MyLLMProvider:
    async def complete(self, messages: list[Message]) -> str:
        # Convert messages to your API format
        formatted = [
            {"role": m.role, "content": m.content}
            for m in messages
        ]
        
        # Call your LLM API
        response = await my_llm_api.generate(formatted)
        
        return response.text
```

### OpenAI-Compatible

```python
from openai import AsyncOpenAI
from icicl import Message

class OpenAIProvider:
    def __init__(
        self,
        model: str = "gpt-4o-mini",
        api_key: str | None = None,
        base_url: str | None = None,
        temperature: float = 0.7,
    ):
        self.model = model
        self.temperature = temperature
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
    
    async def complete(self, messages: list[Message]) -> str:
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": m.role, "content": m.content}
                for m in messages
            ],
            temperature=self.temperature,
        )
        return response.choices[0].message.content or ""
```

### Anthropic Claude

```python
import anthropic
from icicl import Message

class ClaudeProvider:
    def __init__(self, model: str = "claude-3-5-sonnet-20241022"):
        self.model = model
        self.client = anthropic.AsyncAnthropic()
    
    async def complete(self, messages: list[Message]) -> str:
        # Separate system message
        system = None
        chat_messages = []
        
        for m in messages:
            if m.role == "system":
                system = m.content
            else:
                chat_messages.append({
                    "role": m.role,
                    "content": m.content,
                })
        
        response = await self.client.messages.create(
            model=self.model,
            max_tokens=1024,
            system=system,
            messages=chat_messages,
        )
        return response.content[0].text
```

### Local Models (Ollama)

```python
import httpx
from icicl import Message

class OllamaProvider:
    def __init__(
        self,
        model: str = "llama3",
        base_url: str = "http://localhost:11434",
    ):
        self.model = model
        self.base_url = base_url
    
    async def complete(self, messages: list[Message]) -> str:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": [
                        {"role": m.role, "content": m.content}
                        for m in messages
                    ],
                    "stream": False,
                },
                timeout=120.0,
            )
            response.raise_for_status()
            return response.json()["message"]["content"]
```

## Wrappers

### Retry Logic

```python
import asyncio
from icicl import Message

class RetryProvider:
    def __init__(self, base_provider, max_retries: int = 3):
        self.base = base_provider
        self.max_retries = max_retries
    
    async def complete(self, messages: list[Message]) -> str:
        for attempt in range(self.max_retries):
            try:
                return await self.base.complete(messages)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)
```

### Caching

```python
import hashlib
import json
from icicl import Message

class CachedProvider:
    def __init__(self, base_provider):
        self.base = base_provider
        self._cache = {}
    
    def _key(self, messages: list[Message]) -> str:
        data = json.dumps([{"role": m.role, "content": m.content} for m in messages])
        return hashlib.sha256(data.encode()).hexdigest()
    
    async def complete(self, messages: list[Message]) -> str:
        key = self._key(messages)
        
        if key in self._cache:
            return self._cache[key]
        
        result = await self.base.complete(messages)
        self._cache[key] = result
        return result
```

### Fallback

```python
class FallbackProvider:
    def __init__(self, primary, fallback):
        self.primary = primary
        self.fallback = fallback
    
    async def complete(self, messages: list[Message]) -> str:
        try:
            return await self.primary.complete(messages)
        except Exception:
            return await self.fallback.complete(messages)
```

## Mock Provider for Testing

```python
from icicl import Message

class MockProvider:
    def __init__(self):
        self.calls = []
    
    async def complete(self, messages: list[Message]) -> str:
        self.calls.append(messages)
        
        # Return sensible defaults based on prompt
        content = messages[-1].content.lower()
        
        if "plan" in content:
            return "1. First step\n2. Second step"
        elif "reason" in content or "think" in content:
            return "I should proceed with the task."
        else:
            return "execute"
```

## Type Checking

```python
from icicl import LLMProvider

class MyProvider:
    async def complete(self, messages: list[Message]) -> str:
        return "Response"

provider = MyProvider()
print(isinstance(provider, LLMProvider))  # True
```

## Using Custom Providers

```python
from icicl import Agent

# Create your provider
llm = OllamaProvider(model="llama3")

# Or wrap it
llm = RetryProvider(CachedProvider(OllamaProvider(model="llama3")))

# Use with agent
agent = Agent(
    llm=llm,
    db_path="./trajectories",
    plan_prompt="...",
    reason_prompt="...",
    act_prompt="...",
)
```

## See Also

- [LiteLLMProvider](/api-reference/litellm-provider) - Built-in provider
- [Custom LLM Providers Guide](/guides/custom-llm-providers) - Detailed guide
