---
title: "Introduction"
description: "ICRL for Python and TypeScript: self-improving agents that learn from successful trajectories"
---

## What ICRL Is

ICRL (In-Context Reinforcement Learning) is a trajectory-learning framework for LLM agents.

It works by:

1. Running tasks in an environment.
2. Storing successful trajectories.
3. Retrieving similar prior steps during future runs.
4. Curating low-utility trajectories over time.

## What You Get

- Python package: `icrl`
- TypeScript package: `icrl`
- Python CLI: `icrl` (tool-calling coding assistant)
- TypeScript web demo: Next.js + Convex example

## Package Scope

- Python package focuses on:
  - ReAct loop (`Agent`, `ReActLoop`)
  - FAISS-backed `TrajectoryDatabase`
  - Built-in providers (`LiteLLMProvider`, `AnthropicVertexProvider`)
  - CLI and database utilities

- TypeScript package focuses on:
  - Same algorithmic abstractions (`Agent`, `TrajectoryDatabase`, `TrajectoryRetriever`, `CurationManager`)
  - Pluggable storage (`StorageAdapter`), including `FileSystemAdapter`
  - Built-in providers (`OpenAIProvider`, `AnthropicProvider`, `AnthropicVertexProvider`)

## How The Algorithm Runs

1. `reset(goal)` on environment.
2. Generate plan using retrieved examples.
3. Repeat reasoning/action/observation steps.
4. If successful in training mode, store trajectory.
5. Update retrieval feedback and run curation periodically.

## Next Steps

- Start with `/installation`
- Build first run at `/quickstart`
- Read algorithm details at `/core-concepts/icrl-algorithm`
