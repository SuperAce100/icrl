---
title: "Introduction to ICRL"
description: "Build self-improving LLM agents that learn from their own successful experiences using In-Context Reinforcement Learning"
---

<Frame>
  <img
    className="block dark:hidden"
    src="/images/hero-light.svg"
    alt="ICRL architecture diagram showing Bootstrap, Retrieve, Generate, and Curate phases"
  />
  <img
    className="hidden dark:block"
    src="/images/hero-dark.svg"
    alt="ICRL architecture diagram showing Bootstrap, Retrieve, Generate, and Curate phases"
  />
</Frame>

## What is ICRL?

ICRL implements the **In-Context Reinforcement Learning** (ICRL) algorithm, enabling your LLM agents to bootstrap their own performance by learning from successful task completions.

Instead of requiring you to craft examples manually, ICRL agents:

1. **Attempt tasks** and record successful trajectories automatically
2. **Retrieve relevant examples** at each decision point using semantic search
3. **Improve over time** as they accumulate more successful experiences
4. **Self-curate** by pruning low-utility examples from their memory

<CardGroup cols={2}>
  <Card title="Quickstart" icon="rocket" href="/quickstart">
    Get your first ICRL agent running in under 5 minutes
  </Card>
  <Card title="Core Concepts" icon="book" href="/core-concepts/icrl-algorithm">
    Understand how the ICRL algorithm works
  </Card>
  <Card title="API Reference" icon="code" href="/api-reference/overview">
    Explore the complete API documentation
  </Card>
  <Card title="Examples" icon="flask" href="/examples/file-system-agent">
    Learn from real-world implementation examples
  </Card>
</CardGroup>

## Key features

<AccordionGroup>
  <Accordion title="Self-improving agents" icon="brain">
    Your agents automatically learn from successful experiences. Each completed task becomes a potential example for future similar tasks, improving performance without manual intervention.
  </Accordion>
  <Accordion title="Semantic retrieval" icon="magnifying-glass">
    ICRL uses FAISS-backed vector similarity search with sentence transformers to find the most relevant examples for each situation. You configure how many examples to retrieve with the `k` parameter.
  </Accordion>
  <Accordion title="Automatic curation" icon="wand-magic-sparkles">
    Low-utility trajectories are automatically pruned based on their contribution to successful outcomes. You control the aggressiveness of curation through threshold settings.
  </Accordion>
  <Accordion title="100+ LLM support" icon="plug">
    The built-in LiteLLM provider supports OpenAI, Anthropic, Google, Azure, and many other providers through a unified interface.
  </Accordion>
  <Accordion title="Customizable prompts" icon="message">
    Template-based prompts give you complete control over how your agent plans, reasons, and acts.
  </Accordion>
</AccordionGroup>

## How it works

The ICRL algorithm follows four phases:

```
┌─────────────────────────────────────────────────────────────────┐
│                      ICRL Algorithm                             │
├─────────────────────────────────────────────────────────────────┤
│  1. BOOTSTRAP: Agent attempts tasks, stores successes           │
│  2. RETRIEVE: Semantic search finds relevant examples           │
│  3. GENERATE: LLM uses examples for better decisions            │
│  4. CURATE: Low-utility examples are pruned over time          │
└─────────────────────────────────────────────────────────────────┘
```

Each episode follows a **Plan → Reason → Act** loop:

<Steps>
  <Step title="Plan">
    Generate a high-level strategy using retrieved examples from similar past tasks
  </Step>
  <Step title="Reason">
    Analyze the current observation with context from similar situations
  </Step>
  <Step title="Act">
    Execute the next action based on your reasoning
  </Step>
  <Step title="Observe">
    Get environment feedback and loop back to Reason until the task completes
  </Step>
</Steps>

## Research foundation

ICRL implements the algorithm described in:

> **Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks**

<Note>
The research demonstrates that self-generated examples can match or exceed the performance of human-crafted demonstrations while being significantly more scalable.
</Note>

## Quick example

```python quickstart.py
import asyncio
from icrl import Agent, LiteLLMProvider

# Create your agent
agent = Agent(
    llm=LiteLLMProvider(model="gpt-4o-mini"),
    db_path="./trajectories",
    plan_prompt="Goal: {goal}\n\nExamples:\n{examples}\n\nCreate a plan:",
    reason_prompt="Goal: {goal}\nPlan: {plan}\nObservation: {observation}\nThink step by step:",
    act_prompt="Goal: {goal}\nPlan: {plan}\nReasoning: {reasoning}\nNext action:",
    k=3,           # Number of examples to retrieve
    max_steps=30,  # Maximum steps per episode
)

# Training: successful trajectories are stored for future use
trajectory = asyncio.run(agent.train(env, goal="Complete the task"))

# Inference: uses stored examples but doesn't add new ones
trajectory = asyncio.run(agent.run(env, goal="Complete another task"))
```

<Check>
Ready to build your first self-improving agent? Head to the [Quickstart guide](/quickstart) to get started in under 5 minutes.
</Check>
