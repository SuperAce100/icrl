---
title: "Batch Training"
description: "Train your ICRL agents on multiple tasks efficiently to build a robust trajectory database"
---

## Overview

Batch training allows you to train an agent on multiple goals in sequence, building up a database of successful trajectories that improves performance over time.

## Basic batch training

Use `train_batch` to train on multiple goals:

```python batch_training.py
from icrl import Agent, LiteLLMProvider

agent = Agent(
    llm=LiteLLMProvider(model="gpt-4o-mini"),
    db_path="./trajectories",
    plan_prompt="...",
    reason_prompt="...",
    act_prompt="...",
)

# Define a factory that creates fresh environments
def make_env():
    return FileSystemEnvironment()

# Define your training goals
goals = [
    "Navigate to /home/user and list files",
    "Find the config.json file",
    "Read the contents of notes.txt",
    "Copy a file to the backup directory",
]

# Train on all goals
trajectories = await agent.train_batch(make_env, goals)

# Check results
for traj in trajectories:
    status = "✓" if traj.success else "✗"
    print(f"{status} {traj.goal}")
```

## Environment factory pattern

The factory pattern ensures each episode gets a fresh environment:

```python factory.py
def make_file_env():
    """Factory that creates a new FileSystemEnvironment."""
    return FileSystemEnvironment()

# Each goal gets a fresh environment
trajectories = await agent.train_batch(make_file_env, goals)
```

For task-based environments:

```python task_factory.py
from examples.file_api_env import FileSystemEnvironment, Task

tasks = [
    Task(
        goal="Find and read the config file",
        verify=lambda s: "port" in s.last_output,
    ),
    Task(
        goal="Copy notes.txt to /backup",
        verify=lambda s: s.file_exists("/backup/notes.txt"),
    ),
]

# Train on each task
async def train_on_tasks(agent, tasks):
    trajectories = []
    for task in tasks:
        def make_env(t=task):  # Capture task in closure
            return FileSystemEnvironment(t)
        
        traj = await agent.train(make_env(), task.goal)
        trajectories.append(traj)
    
    return trajectories
```

## Curriculum learning

Start with easier tasks and progress to harder ones:

```python curriculum.py
# Define task difficulties
easy_goals = [
    "List files in /home",
    "Print the current directory",
    "Navigate to /etc",
]

medium_goals = [
    "Find all Python files",
    "Read the config.json file",
    "Navigate to /home/user/projects",
]

hard_goals = [
    "Copy config.json to /backup and verify",
    "Find main.py and display its contents",
    "Create a new directory and copy files there",
]

# Train in order of difficulty
print("=== Easy Tasks ===")
await agent.train_batch(make_env, easy_goals)
print(f"Database size: {len(agent.database)}")

print("=== Medium Tasks ===")
await agent.train_batch(make_env, medium_goals)
print(f"Database size: {len(agent.database)}")

print("=== Hard Tasks ===")
await agent.train_batch(make_env, hard_goals)
print(f"Database size: {len(agent.database)}")
```

<Tip>
Curriculum learning helps your agent build foundational skills before tackling complex tasks. Easier tasks create examples that help with harder ones.
</Tip>

## Tracking progress

Monitor training progress with callbacks:

```python progress.py
from rich.console import Console
from rich.progress import Progress

console = Console()

def on_step(step, context):
    console.print(f"  Action: {step.action}")

agent = Agent(..., on_step=on_step)

async def train_with_progress(agent, goals):
    results = {"success": 0, "failed": 0}
    
    with Progress() as progress:
        task = progress.add_task("Training...", total=len(goals))
        
        for goal in goals:
            console.print(f"\n[bold]Goal:[/bold] {goal}")
            
            env = make_env()
            traj = await agent.train(env, goal)
            
            if traj.success:
                results["success"] += 1
                console.print("[green]✓ Success[/green]")
            else:
                results["failed"] += 1
                console.print("[red]✗ Failed[/red]")
            
            progress.update(task, advance=1)
    
    return results

results = await train_with_progress(agent, all_goals)
print(f"\nResults: {results['success']}/{len(all_goals)} succeeded")
```

## Training strategies

<Tabs>
  <Tab title="Bootstrap">
    Train without examples to bootstrap the database:
    
    ```python
    print("Bootstrap phase...")
    await agent.train_batch(make_env, bootstrap_goals)
    
    stats = agent.get_stats()
    print(f"Trajectories stored: {stats['total_trajectories']}")
    ```
  </Tab>
  
  <Tab title="Iterative">
    Run multiple training rounds:
    
    ```python
    for round_num in range(3):
        print(f"\n=== Training Round {round_num + 1} ===")
        
        trajectories = await agent.train_batch(make_env, goals)
        
        successes = sum(1 for t in trajectories if t.success)
        print(f"Success rate: {successes}/{len(goals)}")
        
        if successes == len(goals):
            print("All tasks succeeded! Training complete.")
            break
    ```
  </Tab>
  
  <Tab title="Active Learning">
    Focus on failed tasks:
    
    ```python
    async def active_training(agent, goals, max_rounds=5):
        remaining_goals = list(goals)
        
        for round_num in range(max_rounds):
            if not remaining_goals:
                break
            
            print(f"\n=== Round {round_num + 1}: {len(remaining_goals)} goals ===")
            
            failed = []
            for goal in remaining_goals:
                traj = await agent.train(make_env(), goal)
                if not traj.success:
                    failed.append(goal)
            
            remaining_goals = failed
            print(f"Failed: {len(failed)}")
        
        return len(goals) - len(remaining_goals)
    ```
  </Tab>
</Tabs>

## Batch inference

After training, run inference on new tasks:

```python inference.py
# Freeze the database and evaluate
eval_goals = [
    "New task 1",
    "New task 2",
    "New task 3",
]

trajectories = await agent.run_batch(make_env, eval_goals)

# Analyze results
for traj in trajectories:
    print(f"Goal: {traj.goal}")
    print(f"  Success: {traj.success}")
    print(f"  Steps: {len(traj.steps)}")
```

## Saving and loading progress

The database persists automatically, so you can resume training:

```python resume.py
# Day 1: Initial training
agent = Agent(db_path="./my_project/trajectories", ...)
await agent.train_batch(make_env, day1_goals)
print(f"Day 1 complete: {len(agent.database)} trajectories")

# Day 2: Continue training (database loads automatically)
agent = Agent(db_path="./my_project/trajectories", ...)
print(f"Loaded: {len(agent.database)} trajectories")
await agent.train_batch(make_env, day2_goals)
```

## Monitoring database health

Check the database periodically:

```python health.py
def print_database_health(agent):
    stats = agent.get_stats()
    
    print(f"Total trajectories: {stats['total_trajectories']}")
    print(f"Successful: {stats['successful_trajectories']}")
    print(f"Success rate: {stats['success_rate']:.1%}")
    
    # Check for low-utility trajectories
    low_utility = agent._curation.get_low_utility_trajectories()
    if low_utility:
        print(f"Low utility (will be pruned): {len(low_utility)}")
    
    # Check utility distribution
    scores = agent._curation.get_utility_scores()
    if scores:
        avg_utility = sum(scores.values()) / len(scores)
        print(f"Average utility: {avg_utility:.2f}")

# Call periodically during training
print_database_health(agent)
```

## Complete training pipeline

```python pipeline.py
import asyncio
from icrl import Agent, LiteLLMProvider

async def full_training_pipeline():
    # Initialize
    agent = Agent(
        llm=LiteLLMProvider(model="gpt-4o-mini"),
        db_path="./training_db",
        plan_prompt=PLAN_PROMPT,
        reason_prompt=REASON_PROMPT,
        act_prompt=ACT_PROMPT,
        k=3,
        curation_threshold=0.3,
        curation_min_retrievals=5,
    )
    
    training_goals = [...]
    eval_goals = [...]
    
    # Bootstrap
    print("=== Bootstrap Phase ===")
    for goal in training_goals[:5]:
        traj = await agent.train(make_env(), goal)
        print(f"{'✓' if traj.success else '✗'} {goal[:50]}...")
    
    # Main training
    print("\n=== Main Training ===")
    for round_num in range(3):
        print(f"\nRound {round_num + 1}")
        for goal in training_goals:
            await agent.train(make_env(), goal)
        print_database_health(agent)
    
    # Evaluation
    print("\n=== Evaluation ===")
    results = await agent.run_batch(make_env, eval_goals)
    successes = sum(1 for t in results if t.success)
    print(f"Eval: {successes}/{len(eval_goals)} succeeded")
    
    return agent, results

agent, results = asyncio.run(full_training_pipeline())
```

<Warning>
Parallel training should be used carefully as trajectories are added to the database during training, which could cause race conditions. Use sequential training for reliability.
</Warning>

## Next steps

<CardGroup cols={2}>
  <Card title="Curation" icon="filter" href="/core-concepts/curation">
    Learn about automatic trajectory pruning
  </Card>
  <Card title="File System Example" icon="folder" href="/examples/file-system-agent">
    See batch training in action
  </Card>
</CardGroup>
