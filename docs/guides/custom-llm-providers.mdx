---
title: Custom LLM Providers
description: "Integrating custom LLM backends with ICICL"
---

## Overview

ICICL includes a built-in `LiteLLMProvider` that supports 100+ models. However, you may want to use a custom LLM integration for:

- Self-hosted models
- Custom API endpoints
- Rate limiting or caching
- Specialized inference setups

## The LLMProvider Protocol

Every LLM provider must implement the `LLMProvider` protocol:

```python
from icicl import LLMProvider, Message

class MyLLMProvider:
    async def complete(self, messages: list[Message]) -> str:
        """Generate a completion from the given messages.
        
        Args:
            messages: List of Message(role, content) objects.
        
        Returns:
            Generated text as a string.
        """
        ...
```

## The Message Class

Messages are simple Pydantic models:

```python
from icicl import Message

message = Message(role="user", content="Hello, world!")
# message.role -> "user"
# message.content -> "Hello, world!"
```

Common roles:
- `"user"`: User/human messages
- `"assistant"`: Model responses
- `"system"`: System prompts

## Basic Custom Provider

Here's a simple custom provider:

```python
import httpx
from icicl import LLMProvider, Message

class MyAPIProvider:
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.api_key = api_key
    
    async def complete(self, messages: list[Message]) -> str:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.api_url,
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "messages": [
                        {"role": m.role, "content": m.content}
                        for m in messages
                    ]
                },
            )
            response.raise_for_status()
            return response.json()["content"]
```

## OpenAI-Compatible Provider

For OpenAI-compatible APIs:

```python
import openai
from icicl import Message

class OpenAIProvider:
    def __init__(
        self,
        model: str = "gpt-4o-mini",
        api_key: str | None = None,
        base_url: str | None = None,
        temperature: float = 0.7,
    ):
        self.model = model
        self.temperature = temperature
        self.client = openai.AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
        )
    
    async def complete(self, messages: list[Message]) -> str:
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": m.role, "content": m.content}
                for m in messages
            ],
            temperature=self.temperature,
        )
        return response.choices[0].message.content or ""
```

## Anthropic Provider

For Anthropic's Claude:

```python
import anthropic
from icicl import Message

class AnthropicProvider:
    def __init__(
        self,
        model: str = "claude-3-5-sonnet-20241022",
        api_key: str | None = None,
        max_tokens: int = 1024,
    ):
        self.model = model
        self.max_tokens = max_tokens
        self.client = anthropic.AsyncAnthropic(api_key=api_key)
    
    async def complete(self, messages: list[Message]) -> str:
        # Separate system message if present
        system = None
        chat_messages = []
        
        for m in messages:
            if m.role == "system":
                system = m.content
            else:
                chat_messages.append({
                    "role": m.role,
                    "content": m.content,
                })
        
        response = await self.client.messages.create(
            model=self.model,
            max_tokens=self.max_tokens,
            system=system,
            messages=chat_messages,
        )
        return response.content[0].text
```

## Local Model Provider (Ollama)

For locally-hosted models via Ollama:

```python
import httpx
from icicl import Message

class OllamaProvider:
    def __init__(
        self,
        model: str = "llama3",
        base_url: str = "http://localhost:11434",
    ):
        self.model = model
        self.base_url = base_url
    
    async def complete(self, messages: list[Message]) -> str:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": [
                        {"role": m.role, "content": m.content}
                        for m in messages
                    ],
                    "stream": False,
                },
                timeout=120.0,
            )
            response.raise_for_status()
            return response.json()["message"]["content"]
```

## vLLM Provider

For high-performance inference with vLLM:

```python
from openai import AsyncOpenAI
from icicl import Message

class VLLMProvider:
    def __init__(
        self,
        model: str,
        base_url: str = "http://localhost:8000/v1",
    ):
        self.model = model
        self.client = AsyncOpenAI(
            api_key="EMPTY",  # vLLM doesn't require a real key
            base_url=base_url,
        )
    
    async def complete(self, messages: list[Message]) -> str:
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": m.role, "content": m.content}
                for m in messages
            ],
        )
        return response.choices[0].message.content or ""
```

## Provider with Caching

Add caching to reduce API calls:

```python
import hashlib
import json
from functools import lru_cache
from icicl import Message

class CachedProvider:
    def __init__(self, base_provider, cache_size: int = 1000):
        self.base_provider = base_provider
        self._cache = {}
        self._cache_size = cache_size
    
    def _cache_key(self, messages: list[Message]) -> str:
        content = json.dumps([
            {"role": m.role, "content": m.content}
            for m in messages
        ])
        return hashlib.sha256(content.encode()).hexdigest()
    
    async def complete(self, messages: list[Message]) -> str:
        key = self._cache_key(messages)
        
        if key in self._cache:
            return self._cache[key]
        
        result = await self.base_provider.complete(messages)
        
        # Simple LRU-like behavior
        if len(self._cache) >= self._cache_size:
            # Remove oldest entry
            oldest = next(iter(self._cache))
            del self._cache[oldest]
        
        self._cache[key] = result
        return result
```

## Provider with Retry Logic

Handle transient failures:

```python
import asyncio
from icicl import Message

class RetryProvider:
    def __init__(
        self,
        base_provider,
        max_retries: int = 3,
        base_delay: float = 1.0,
    ):
        self.base_provider = base_provider
        self.max_retries = max_retries
        self.base_delay = base_delay
    
    async def complete(self, messages: list[Message]) -> str:
        last_exception = None
        
        for attempt in range(self.max_retries):
            try:
                return await self.base_provider.complete(messages)
            except Exception as e:
                last_exception = e
                if attempt < self.max_retries - 1:
                    delay = self.base_delay * (2 ** attempt)
                    await asyncio.sleep(delay)
        
        raise last_exception
```

## Mock Provider for Testing

Test without making API calls:

```python
from icicl import Message

class MockProvider:
    def __init__(self, responses: dict[str, str] | None = None):
        self.responses = responses or {}
        self.calls = []
    
    async def complete(self, messages: list[Message]) -> str:
        self.calls.append(messages)
        
        # Check for matching response
        last_content = messages[-1].content.lower()
        for pattern, response in self.responses.items():
            if pattern in last_content:
                return response
        
        # Default responses based on prompt type
        if "plan" in last_content:
            return "1. First step\n2. Second step"
        elif "reason" in last_content or "think" in last_content:
            return "I should proceed with the next step."
        else:
            return "action"
```

## Combining Providers

Create composite providers:

```python
class FallbackProvider:
    def __init__(self, primary, fallback):
        self.primary = primary
        self.fallback = fallback
    
    async def complete(self, messages: list[Message]) -> str:
        try:
            return await self.primary.complete(messages)
        except Exception:
            return await self.fallback.complete(messages)

class LoadBalancedProvider:
    def __init__(self, providers: list):
        self.providers = providers
        self.index = 0
    
    async def complete(self, messages: list[Message]) -> str:
        provider = self.providers[self.index]
        self.index = (self.index + 1) % len(self.providers)
        return await provider.complete(messages)
```

## Using Custom Providers

```python
from icicl import Agent

# Create your custom provider
llm = OllamaProvider(model="llama3")

# Or with wrappers
llm = RetryProvider(
    CachedProvider(
        OllamaProvider(model="llama3")
    )
)

# Use with agent
agent = Agent(
    llm=llm,
    db_path="./trajectories",
    plan_prompt="...",
    reason_prompt="...",
    act_prompt="...",
)
```

## Synchronous Support

Add sync methods for convenience:

```python
import asyncio

class MyProvider:
    async def complete(self, messages: list[Message]) -> str:
        # Async implementation
        ...
    
    def complete_sync(self, messages: list[Message]) -> str:
        """Synchronous wrapper."""
        return asyncio.run(self.complete(messages))
```

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Batch Training"
    icon="layer-group"
    href="/guides/batch-training"
  >
    Train on multiple tasks efficiently
  </Card>
  <Card
    title="API Reference"
    icon="code"
    href="/api-reference/llm-provider"
  >
    Complete LLMProvider API documentation
  </Card>
</CardGroup>
